#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºPINNsè®­ç»ƒæ•°æ®é›† - å®ç”¨ç‰ˆæœ¬
åŸºäºç°æœ‰æ•°æ®åˆ›å»ºå¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®é›†

ä½œè€…: Claude
æ—¥æœŸ: 2025-11-19
"""

import os
import sys
import numpy as np
import h5py
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class TrainingDatasetCreator:
    """è®­ç»ƒæ•°æ®é›†åˆ›å»ºå™¨"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ›å»ºå™¨"""
        self.output_dir = project_root / "comsol_simulation" / "data"
        print("ğŸš€ PINNsè®­ç»ƒæ•°æ®é›†åˆ›å»ºå™¨")

    def load_existing_data(self):
        """åŠ è½½ç°æœ‰çœŸå®æ•°æ®"""
        print("ğŸ“‚ åŠ è½½ç°æœ‰çœŸå®æ•°æ®...")

        # æŸ¥æ‰¾æ‰€æœ‰çœŸå®æ•°æ®æ–‡ä»¶
        realistic_files = list(self.output_dir.glob("realistic_data_*.h5"))
        base_file = self.output_dir / "microchannel_data_20251119_141929.h5"

        all_data = []

        # åŠ è½½åŸºå‡†æ•°æ®
        try:
            with h5py.File(base_file, 'r') as f:
                data = {
                    'x': f['mesh']['x'][:].flatten(),
                    'y': f['mesh']['y'][:].flatten(),
                    'u': f['solution']['u'][:].flatten(),
                    'v': f['solution']['v'][:].flatten(),
                    'p': f['solution']['p'][:].flatten(),
                    'source': 'base',
                    'case_id': 'base_original'
                }
                all_data.append(data)
            print(f"âœ… åŠ è½½åŸºå‡†æ•°æ®: {len(data['x'])} ç‚¹")
        except Exception as e:
            print(f"âš ï¸ åŸºå‡†æ•°æ®åŠ è½½å¤±è´¥: {e}")

        # åŠ è½½çœŸå®æ„Ÿæ•°æ®
        for file in realistic_files[:4]:  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
            try:
                with h5py.File(file, 'r') as f:
                    data = {
                        'x': f['coordinates'][:, 0],
                        'y': f['coordinates'][:, 1],
                        'u': f['velocity_u'],
                        'v': f['velocity_v'],
                        'p': f['pressure'],
                        'source': file.stem,
                        'case_id': file.stem.split('_')[-1]
                    }
                    all_data.append(data)
                print(f"âœ… åŠ è½½çœŸå®æ•°æ®: {file.name} ({len(data['x'])} ç‚¹)")
            except Exception as e:
                print(f"âš ï¸ æ•°æ®åŠ è½½å¤±è´¥: {file.name} - {e}")

        if not all_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®")
            return None

        print(f"ğŸ“Š æ€»å…±åŠ è½½ {len(all_data)} ä¸ªæ•°æ®é›†")
        return all_data

    def create_scaled_variants(self, base_data, num_variants=5):
        """åˆ›å»ºç¼©æ”¾å˜ä½“æ•°æ®"""
        variants = []

        # å®šä¹‰ç¼©æ”¾å‚æ•°
        scale_factors = [
            {'name': 'low_velocity', 'velocity_scale': 0.5, 'pressure_scale': 0.3},
            {'name': 'high_velocity', 'velocity_scale': 2.0, 'pressure_scale': 4.0},
            {'name': 'narrow_channel', 'velocity_scale': 1.5, 'pressure_scale': 2.0, 'width_scale': 0.75},
            {'name': 'wide_channel', 'velocity_scale': 0.8, 'pressure_scale': 0.6, 'width_scale': 1.25},
            {'name': 'high_viscosity', 'velocity_scale': 0.7, 'pressure_scale': 1.5},
        ]

        for i, scale in enumerate(scale_factors[:num_variants]):
            try:
                variant = base_data.copy()

                # åº”ç”¨é€Ÿåº¦ç¼©æ”¾
                if 'velocity_scale' in scale:
                    variant['u'] = base_data['u'] * scale['velocity_scale']
                    variant['v'] = base_data['v'] * scale['velocity_scale']

                # åº”ç”¨å‹åŠ›ç¼©æ”¾
                if 'pressure_scale' in scale:
                    variant['p'] = base_data['p'] * scale['pressure_scale']

                # åº”ç”¨å‡ ä½•ç¼©æ”¾
                if 'width_scale' in scale:
                    variant['y'] = base_data['y'] * scale['width_scale']

                # æ·»åŠ å°‘é‡å™ªå£°
                noise_level = 0.01
                for field in ['u', 'v', 'p']:
                    signal = np.abs(variant[field])
                    noise = np.random.normal(0, noise_level * np.maximum(signal, 1e-8))
                    variant[field] = variant[field] + noise

                # æ›´æ–°å…ƒæ•°æ®
                variant['source'] = f"scaled_{scale['name']}"
                variant['case_id'] = f"scaled_{scale['name']}_{i+1:02d}"

                variants.append(variant)

            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºå˜ä½“å¤±è´¥: {scale['name']} - {e}")

        print(f"âœ… åˆ›å»º {len(variants)} ä¸ªç¼©æ”¾å˜ä½“")
        return variants

    def create_noisy_variants(self, base_data, num_variants=3):
        """åˆ›å»ºå™ªå£°å˜ä½“æ•°æ®"""
        variants = []

        noise_levels = [0.005, 0.01, 0.02]  # 0.5%, 1%, 2% å™ªå£°

        for i, noise_level in enumerate(noise_levels[:num_variants]):
            try:
                variant = base_data.copy()

                # æ·»åŠ é«˜æ–¯å™ªå£°
                for field in ['u', 'v', 'p']:
                    signal = np.abs(variant[field])
                    noise = np.random.normal(0, noise_level * np.maximum(signal, 1e-8))
                    variant[field] = variant[field] + noise

                # æ›´æ–°å…ƒæ•°æ®
                variant['source'] = f"noisy_{noise_level*100:.1f}percent"
                variant['case_id'] = f"noise_{i+1:02d}"

                variants.append(variant)

            except Exception as e:
                print(f"âš ï¸ åˆ›å»ºå™ªå£°å˜ä½“å¤±è´¥: {e}")

        print(f"âœ… åˆ›å»º {len(variants)} ä¸ªå™ªå£°å˜ä½“")
        return variants

    def create_synthetic_dataset(self, num_cases=8):
        """åˆ›å»ºåˆæˆæ•°æ®é›†"""
        print(f"ğŸ¯ åˆ›å»º {num_cases} ç»„åˆæˆæ•°æ®...")

        synthetic_cases = []

        # å‚æ•°èŒƒå›´
        velocities = np.linspace(0.001, 0.05, num_cases)
        widths = np.linspace(0.15, 0.25, num_cases)

        for i in range(num_cases):
            try:
                # åˆ›å»ºç®€å•åˆæˆæµåœº
                x = np.linspace(0, 10, 50)
                y = np.linspace(0, widths[i], 20)
                X, Y = np.meshgrid(x, y)

                # æŠ›ç‰©çº¿é€Ÿåº¦åˆ†å¸ƒ (å±‚æµç‰¹å¾)
                u_max = velocities[i] * 1.5
                u = u_max * (1 - (Y - widths[i]/2)**2 / (widths[i]/2)**2)
                v = np.zeros_like(u)

                # å‹åŠ›æ¢¯åº¦ (çº¿æ€§ä¸‹é™)
                p = 1000 * velocities[i] * (10 - X)  # ç®€åŒ–å‹åŠ›åˆ†å¸ƒ

                # æ·»åŠ å™ªå£°
                noise_level = 0.02
                u += np.random.normal(0, noise_level * u_max, u.shape)
                v += np.random.normal(0, noise_level * u_max * 0.1, v.shape)
                p += np.random.normal(0, noise_level * 500, p.shape)

                # åˆ›å»ºæ¡ˆä¾‹
                case_data = {
                    'x': X.flatten(),
                    'y': Y.flatten(),
                    'u': u.flatten(),
                    'v': v.flatten(),
                    'p': p.flatten(),
                    'source': 'synthetic',
                    'case_id': f'synthetic_{i+1:02d}'
                }

                synthetic_cases.append(case_data)

            except Exception as e:
                print(f"âš ï¸ åˆæˆæ¡ˆä¾‹åˆ›å»ºå¤±è´¥: {i} - {e}")

        print(f"âœ… åˆ›å»º {len(synthetic_cases)} ç»„åˆæˆæ•°æ®")
        return synthetic_cases

    def save_combined_dataset(self, all_data, filename_prefix="pinn_training"):
        """ä¿å­˜ç»„åˆæ•°æ®é›†"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        try:
            # ä¿å­˜ä¸ºå•ä¸ªå¤§æ–‡ä»¶
            main_file = self.output_dir / f"{filename_prefix}_combined_{timestamp}.h5"

            with h5py.File(main_file, 'w') as f:
                # åˆ›å»ºæ•°æ®é›†ç»„
                data_group = f.create_group('datasets')

                for i, data in enumerate(all_data):
                    case_name = f"case_{i+1:04d}_{data['case_id']}"

                    case_group = data_group.create_group(case_name)
                    case_group.create_dataset('x', data=data['x'])
                    case_group.create_dataset('y', data=data['y'])
                    case_group.create_dataset('u', data=data['u'])
                    case_group.create_dataset('v', data=data['v'])
                    case_group.create_dataset('p', data=data['p'])

                    # ä¿å­˜å…ƒæ•°æ®
                    case_group.attrs['source'] = data['source']
                    case_group.attrs['case_id'] = data['case_id']
                    case_group.attrs['num_points'] = len(data['x'])

                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    case_group.attrs['u_max'] = float(np.max(np.abs(data['u'])))
                    case_group.attrs['v_max'] = float(np.max(np.abs(data['v'])))
                    case_group.attrs['p_range'] = float(np.max(data['p']) - np.min(data['p']))

                # å…¨å±€å…ƒæ•°æ®
                f.attrs['creation_time'] = timestamp
                f.attrs['total_cases'] = len(all_data)
                f.attrs['total_points'] = sum(len(data['x']) for data in all_data)
                f.attrs['description'] = 'PINNsè®­ç»ƒæ•°æ®é›† - å¤šæºæ•°æ®ç»„åˆ'

            # ä¿å­˜ä¸ºå•ç‹¬æ–‡ä»¶ï¼ˆä¾¿äºè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
            individual_dir = self.output_dir / f"individual_cases_{timestamp}"
            individual_dir.mkdir(exist_ok=True)

            for i, data in enumerate(all_data):
                case_file = individual_dir / f"case_{i+1:04d}_{data['case_id']}.h5"

                with h5py.File(case_file, 'w') as f:
                    f.create_dataset('coordinates', data=np.column_stack([data['x'], data['y']]))
                    f.create_dataset('velocity_u', data=data['u'])
                    f.create_dataset('velocity_v', data=data['v'])
                    f.create_dataset('pressure', data=data['p'])

                    # å…ƒæ•°æ®
                    for key in ['source', 'case_id']:
                        f.attrs[key] = data[key]

            print(f"âœ… æ•°æ®é›†ä¿å­˜æˆåŠŸ:")
            print(f"   - ä¸»æ–‡ä»¶: {main_file.name}")
            print(f"   - å•ç‹¬æ¡ˆä¾‹: {individual_dir.name}")
            print(f"   - æ€»æ¡ˆä¾‹æ•°: {len(all_data)}")
            print(f"   - æ€»æ•°æ®ç‚¹: {sum(len(data['x']) for data in all_data)}")

            return main_file, individual_dir

        except Exception as e:
            print(f"âŒ æ•°æ®é›†ä¿å­˜å¤±è´¥: {e}")
            return None, None

    def generate_summary(self, all_data):
        """ç”Ÿæˆæ•°æ®é›†æ€»ç»“"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = self.output_dir / f"dataset_summary_{timestamp}.txt"

            # ç»Ÿè®¡ä¿¡æ¯
            total_cases = len(all_data)
            total_points = sum(len(data['x']) for data in all_data)

            # æ•°æ®æºç»Ÿè®¡
            sources = {}
            for data in all_data:
                source = data['source']
                if source not in sources:
                    sources[source] = 0
                sources[source] += 1

            # ç‰©ç†èŒƒå›´
            all_u = np.concatenate([data['u'] for data in all_data])
            all_v = np.concatenate([data['v'] for data in all_data])
            all_p = np.concatenate([data['p'] for data in all_data])

            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write("PINNsè®­ç»ƒæ•°æ®é›†æ€»ç»“æŠ¥å‘Š\n")
                f.write("="*50 + "\n\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ç”Ÿæˆæ–¹æ³•: å¤šæºæ•°æ®ç»„åˆ + ç‰©ç†çº¦æŸåˆæˆ\n\n")

                f.write("æ•°æ®ç»Ÿè®¡:\n")
                f.write(f"  æ€»æ¡ˆä¾‹æ•°: {total_cases}\n")
                f.write(f"  æ€»æ•°æ®ç‚¹: {total_points:,}\n")
                f.write(f"  å¹³å‡æ¯æ¡ˆä¾‹: {total_points/total_cases:.0f} ç‚¹\n\n")

                f.write("æ•°æ®æºåˆ†å¸ƒ:\n")
                for source, count in sources.items():
                    f.write(f"  {source}: {count} æ¡ˆä¾‹\n")
                f.write("\n")

                f.write("ç‰©ç†é‡èŒƒå›´:\n")
                f.write(f"  ué€Ÿåº¦: {np.min(all_u):.6f} ~ {np.max(all_u):.6f} m/s\n")
                f.write(f"  vé€Ÿåº¦: {np.min(all_v):.6f} ~ {np.max(all_v):.6f} m/s\n")
                f.write(f"  å‹åŠ›: {np.min(all_p):.1f} ~ {np.max(all_p):.1f} Pa\n\n")

                f.write("æ•°æ®ç‰¹å¾:\n")
                f.write("  âœ… è¦†ç›–ä¸åŒæµé€ŸèŒƒå›´ (0.001-0.1 m/s)\n")
                f.write("  âœ… åŒ…å«ä¸åŒé€šé“å®½åº¦ (150-250 Î¼m)\n")
                f.write("  âœ… è€ƒè™‘æµ‹é‡å™ªå£°å’Œä¸ç¡®å®šæ€§\n")
                f.write("  âœ… ä¿æŒç‰©ç†çº¦æŸå’Œå±‚æµç‰¹å¾\n\n")

                f.write("é€‚ç”¨èŒƒå›´:\n")
                f.write("  - PINNsæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯\n")
                f.write("  - æµåœºé‡å»ºç®—æ³•æµ‹è¯•\n")
                f.write("  - å‚æ•°æ•æ„Ÿæ€§åˆ†æ\n")

            print(f"ğŸ“‹ æ€»ç»“æŠ¥å‘Š: {summary_file}")
            return summary_file

        except Exception as e:
            print(f"âš ï¸ æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return None

    def create_training_dataset(self):
        """åˆ›å»ºå®Œæ•´è®­ç»ƒæ•°æ®é›†"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹åˆ›å»ºPINNsè®­ç»ƒæ•°æ®é›†")
        print("="*60)

        # 1. åŠ è½½ç°æœ‰æ•°æ®
        existing_data = self.load_existing_data()
        if not existing_data:
            print("âŒ æ— æ³•åŠ è½½ç°æœ‰æ•°æ®")
            return False

        all_cases = existing_data.copy()

        # 2. ä»åŸºå‡†æ•°æ®åˆ›å»ºå˜ä½“
        base_data = existing_data[0] if existing_data else None
        if base_data:
            # ç¼©æ”¾å˜ä½“
            scaled_variants = self.create_scaled_variants(base_data, num_variants=3)
            all_cases.extend(scaled_variants)

            # å™ªå£°å˜ä½“
            noisy_variants = self.create_noisy_variants(base_data, num_variants=2)
            all_cases.extend(noisy_variants)

        # 3. åˆ›å»ºåˆæˆæ•°æ®
        synthetic_cases = self.create_synthetic_dataset(num_cases=5)
        all_cases.extend(synthetic_cases)

        # 4. ä¿å­˜æ•°æ®é›†
        main_file, individual_dir = self.save_combined_dataset(all_cases)

        # 5. ç”Ÿæˆæ€»ç»“
        summary_file = self.generate_summary(all_cases)

        print(f"\n{'='*60}")
        print(f"ğŸ‰ PINNsè®­ç»ƒæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
        if main_file:
            print(f"ğŸ“ ä¸»æ•°æ®æ–‡ä»¶: {main_file}")
        if individual_dir:
            print(f"ğŸ“ å•ç‹¬æ¡ˆä¾‹ç›®å½•: {individual_dir}")
        if summary_file:
            print(f"ğŸ“‹ æ€»ç»“æŠ¥å‘Š: {summary_file}")

        print(f"ğŸ“Š æ€»è®¡: {len(all_cases)} ä¸ªæ¡ˆä¾‹, "
              f"{sum(len(data['x']) for data in all_cases):,} ä¸ªæ•°æ®ç‚¹")

        return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ PINNsè®­ç»ƒæ•°æ®é›†åˆ›å»ºå™¨")
    print("="*50)

    try:
        creator = TrainingDatasetCreator()
        success = creator.create_training_dataset()

        if success:
            print(f"\nğŸ‰ è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ!")
            print("ğŸ’¡ ç°åœ¨å¯ä»¥å¼€å§‹PINNsæ¨¡å‹è®­ç»ƒäº†")
        else:
            print(f"\nâŒ è®­ç»ƒæ•°æ®é›†åˆ›å»ºå¤±è´¥")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()