"""
çœŸå®æ•°æ®é›†éªŒè¯å’Œåˆ†æå·¥å…·

ç”¨äºæ£€æŸ¥ç”Ÿæˆçš„çœŸå®æ•°æ®é›†çš„è´¨é‡å’Œæœ‰æ•ˆæ€§
æä¾›å¤šç§éªŒè¯æŒ‡æ ‡å’Œå¯è§†åŒ–æ–¹æ³•

ä½œè€…: PINNsé¡¹ç›®ç»„
åˆ›å»ºæ—¶é—´: 2025-11-19
"""

import os
import sys
import numpy as np
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple
# import seaborn as sns  # å¯é€‰ï¼Œå¦‚æœéœ€è¦æ›´é«˜çº§çš„å¯è§†åŒ–

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


class RealisticDataValidator:
    """çœŸå®æ•°æ®é›†éªŒè¯å™¨"""

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–éªŒè¯å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        if data_dir is None:
            data_dir = project_root / "comsol_simulation" / "data"

        self.data_dir = Path(data_dir)

    def load_realistic_dataset(self, filename: str) -> Dict:
        """
        åŠ è½½çœŸå®æ•°æ®é›†

        Args:
            filename: æ•°æ®é›†æ–‡ä»¶å

        Returns:
            dict: æ•°æ®é›†å­—å…¸
        """
        file_path = self.data_dir / filename

        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        print(f"ğŸ“ åŠ è½½çœŸå®æ•°æ®é›†: {filename}")

        with h5py.File(file_path, 'r') as h5file:
            data = {}

            # åŠ è½½åŸºæœ¬ä¿¡æ¯
            info_group = h5file.get('info')
            if info_group:
                data['info'] = dict(info_group.attrs)

            # åŠ è½½ç½‘æ ¼æ•°æ®
            mesh_group = h5file.get('mesh')
            if mesh_group:
                data['mesh'] = {
                    'x': mesh_group['x'][:],
                    'y': mesh_group['y'][:],
                    'num_nodes': mesh_group.attrs.get('num_nodes', len(mesh_group['x']))  # æ·»åŠ é»˜è®¤å€¼
                }

            # åŠ è½½æ±‚è§£æ•°æ®ï¼ˆåŒ…å«å™ªå£°ï¼‰
            solution_group = h5file.get('solution')
            if solution_group:
                data['solution'] = {
                    'u': solution_group['u'][:],
                    'v': solution_group['v'][:],
                    'p': solution_group['p'][:],
                    'u_clean': solution_group['u_clean'][:],
                    'v_clean': solution_group['v_clean'][:],
                    'p_clean': solution_group['p_clean'][:],
                    'missing_mask': solution_group['missing_mask'][:]
                }

            # åŠ è½½é‡‡æ ·ä¿¡æ¯
            sampling_group = h5file.get('sampling')
            if sampling_group:
                data['sampling'] = dict(sampling_group.attrs)

            # åŠ è½½å™ªå£°åˆ†æ
            noise_group = h5file.get('noise_analysis')
            if noise_group:
                data['noise_analysis'] = {}
                for field in ['u', 'v', 'p']:
                    if field in noise_group:
                        data['noise_analysis'][field] = dict(noise_group[field].attrs)

        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
        return data

    def validate_data_integrity(self, data: Dict) -> Dict:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§

        Args:
            data: æ•°æ®é›†å­—å…¸

        Returns:
            dict: éªŒè¯ç»“æœ
        """
        print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")

        validation_results = {}

        # 1. æ£€æŸ¥æ•°æ®å½¢çŠ¶
        mesh_points = len(data['mesh']['x'])
        solution_points = len(data['solution']['u'])

        validation_results['shape_consistency'] = {
            'mesh_points': mesh_points,
            'solution_points': solution_points,
            'consistent': mesh_points == solution_points
        }

        # 2. æ£€æŸ¥ç¼ºå¤±æ•°æ®
        missing_mask = data['solution']['missing_mask']
        missing_count = np.sum(missing_mask)
        missing_ratio = missing_count / len(missing_mask)

        validation_results['missing_data'] = {
            'missing_count': int(missing_count),
            'missing_ratio': float(missing_ratio),
            'total_points': len(missing_mask)
        }

        # 3. æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
        u_clean = data['solution']['u_clean']
        v_clean = data['solution']['v_clean']
        p_clean = data['solution']['p_clean']

        # æ£€æŸ¥é€Ÿåº¦èŒƒå›´ï¼ˆå¾®æµæ§é€šå¸¸0-0.1 m/sï¼‰
        speed_clean = np.sqrt(u_clean**2 + v_clean**2)
        validation_results['velocity_range'] = {
            'u_min': float(np.min(u_clean)),
            'u_max': float(np.max(u_clean)),
            'v_min': float(np.min(v_clean)),
            'v_max': float(np.max(v_clean)),
            'speed_max': float(np.max(speed_clean)),
            'reasonable': np.max(speed_clean) < 0.1  # 10cm/sä¸Šé™
        }

        # 4. æ£€æŸ¥å‹åŠ›èŒƒå›´ï¼ˆå¾®æµæ§é€šå¸¸-10kPaåˆ°10kPaï¼‰
        validation_results['pressure_range'] = {
            'p_min': float(np.min(p_clean)),
            'p_max': float(np.max(p_clean)),
            'range': float(np.max(p_clean) - np.min(p_clean)),
            'reasonable': abs(np.max(p_clean) - np.min(p_clean)) < 20000  # 20kPaä¸Šé™
        }

        # 5. æ£€æŸ¥å™ªå£°æ°´å¹³
        if 'noise_analysis' in data:
            noise_analysis = data['noise_analysis']
            validation_results['noise_levels'] = {}
            for field in ['u', 'v', 'p']:
                if field in noise_analysis:
                    validation_results['noise_levels'][field] = {
                        'snr_db': noise_analysis[field].get('snr_db', 0),
                        'noise_std': noise_analysis[field].get('noise_std', 0)
                    }

        # 6. æ£€æŸ¥é‡‡æ ·ä¿¡æ¯
        if 'sampling' in data:
            sampling = data['sampling']
            validation_results['sampling_info'] = {
                'strategy': sampling.get('strategy', 'unknown'),
                'original_points': sampling.get('original_points', 0),
                'sampled_points': sampling.get('sampled_points', 0),
                'sampling_ratio': sampling.get('sampling_ratio', 0)
            }
        else:
            # å¦‚æœæ²¡æœ‰é‡‡æ ·ä¿¡æ¯ï¼Œæä¾›é»˜è®¤å€¼
            validation_results['sampling_info'] = {
                'strategy': 'unknown',
                'original_points': mesh_points,
                'sampled_points': mesh_points,
                'sampling_ratio': 1.0
            }

        print("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
        return validation_results

    def generate_validation_report(self, filename: str) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š

        Args:
            filename: æ•°æ®é›†æ–‡ä»¶å

        Returns:
            dict: å®Œæ•´çš„éªŒè¯æŠ¥å‘Š
        """
        print(f"ğŸ“‹ ç”ŸæˆéªŒè¯æŠ¥å‘Š: {filename}")

        # åŠ è½½æ•°æ®
        data = self.load_realistic_dataset(filename)

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        validation_results = self.validate_data_integrity(data)

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'dataset_info': {
                'filename': filename,
                'file_size_mb': round(os.path.getsize(self.data_dir / filename) / (1024*1024), 2),
                'creation_time': data['info'].get('creation_time', 'unknown'),
                'data_type': data['info'].get('data_type', 'unknown')
            },
            'validation_results': validation_results
        }

        return report, data

    def visualize_data_validation(self, data: Dict, validation_results: Dict,
                                save_path: str = None):
        """
        å¯è§†åŒ–æ•°æ®éªŒè¯ç»“æœ

        Args:
            data: æ•°æ®é›†å­—å…¸
            validation_results: éªŒè¯ç»“æœ
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'çœŸå®æ•°æ®é›†éªŒè¯åˆ†æ - {data["info"].get("data_type", "Unknown")}',
                    fontsize=16)

        # æå–æ•°æ®
        x = data['mesh']['x']
        y = data['mesh']['y']
        u_clean = data['solution']['u_clean']
        v_clean = data['solution']['v_clean']
        p_clean = data['solution']['p_clean']
        u_noisy = data['solution']['u']
        v_noisy = data['solution']['v']
        p_noisy = data['solution']['p']
        missing_mask = data['solution']['missing_mask']

        # 1. é‡‡æ ·ç‚¹åˆ†å¸ƒ
        ax1 = axes[0, 0]
        scatter = ax1.scatter(x[~missing_mask], y[~missing_mask],
                            c='blue', s=10, alpha=0.6, label='æœ‰æ•ˆæ•°æ®ç‚¹')
        if np.any(missing_mask):
            ax1.scatter(x[missing_mask], y[missing_mask],
                       c='red', s=10, alpha=0.6, label='ç¼ºå¤±æ•°æ®ç‚¹')
        ax1.set_xlabel('X (mm)')
        ax1.set_ylabel('Y (mm)')
        ax1.set_title(f'é‡‡æ ·ç‚¹åˆ†å¸ƒ ({validation_results["sampling_info"]["strategy"]})')
        ax1.legend()
        ax1.set_aspect('equal')

        # 2. é€Ÿåº¦åœºå¯¹æ¯”ï¼ˆå¹²å‡€ vs å™ªå£°ï¼‰
        ax2 = axes[0, 1]
        speed_clean = np.sqrt(u_clean**2 + v_clean**2)
        speed_noisy = np.sqrt(u_noisy**2 + v_noisy**2)

        # åªæ˜¾ç¤ºæœ‰æ•ˆæ•°æ®ç‚¹
        valid_mask = ~missing_mask
        scatter = ax2.scatter(speed_clean[valid_mask], speed_noisy[valid_mask],
                            alpha=0.6, s=10)

        # æ·»åŠ y=xå‚è€ƒçº¿
        max_speed = max(np.max(speed_clean), np.max(speed_noisy))
        ax2.plot([0, max_speed], [0, max_speed], 'r--', alpha=0.8, label='y=x (å®Œç¾åŒ¹é…)')
        ax2.set_xlabel('å¹²å‡€é€Ÿåº¦å¹…å€¼ (m/s)')
        ax2.set_ylabel('å™ªå£°é€Ÿåº¦å¹…å€¼ (m/s)')
        ax2.set_title('é€Ÿåº¦åœºå™ªå£°å½±å“')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. å‹åŠ›åœºå¯¹æ¯”ï¼ˆå¹²å‡€ vs å™ªå£°ï¼‰
        ax3 = axes[0, 2]
        valid_mask = ~missing_mask
        scatter = ax3.scatter(p_clean[valid_mask], p_noisy[valid_mask],
                            alpha=0.6, s=10, c='orange')

        # æ·»åŠ y=xå‚è€ƒçº¿
        min_p, max_p = min(np.min(p_clean), np.min(p_noisy)), max(np.max(p_clean), np.max(p_noisy))
        ax3.plot([min_p, max_p], [min_p, max_p], 'r--', alpha=0.8, label='y=x (å®Œç¾åŒ¹é…)')
        ax3.set_xlabel('å¹²å‡€å‹åŠ› (Pa)')
        ax3.set_ylabel('å™ªå£°å‹åŠ› (Pa)')
        ax3.set_title('å‹åŠ›åœºå™ªå£°å½±å“')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. å™ªå£°ç»Ÿè®¡åˆ†æ
        ax4 = axes[1, 0]
        if 'noise_levels' in validation_results:
            fields = list(validation_results['noise_levels'].keys())
            snr_values = [validation_results['noise_levels'][field]['snr_db'] for field in fields]

            bars = ax4.bar(fields, snr_values, alpha=0.7,
                          color=['skyblue', 'lightgreen', 'salmon'])
            ax4.set_ylabel('ä¿¡å™ªæ¯” (dB)')
            ax4.set_title('å„ç‰©ç†é‡ä¿¡å™ªæ¯”')
            ax4.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, snr in zip(bars, snr_values):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{snr:.1f} dB', ha='center', va='bottom')

        # 5. æ•°æ®è´¨é‡æŒ‡æ ‡
        ax5 = axes[1, 1]
        quality_metrics = [
            ('æ•°æ®å®Œæ•´æ€§', f"{(1-validation_results['missing_data']['missing_ratio'])*100:.1f}%"),
            ('é€Ÿåº¦åˆç†æ€§', 'âœ“' if validation_results['velocity_range']['reasonable'] else 'âœ—'),
            ('å‹åŠ›åˆç†æ€§', 'âœ“' if validation_results['pressure_range']['reasonable'] else 'âœ—'),
            ('å½¢çŠ¶ä¸€è‡´æ€§', 'âœ“' if validation_results['shape_consistency']['consistent'] else 'âœ—')
        ]

        y_pos = np.arange(len(quality_metrics))
        colors = ['green' if 'âœ“' in metric[1] else 'red' for metric in quality_metrics]

        bars = ax5.barh(y_pos, [1]*len(quality_metrics), color=colors, alpha=0.7)
        ax5.set_yticks(y_pos)
        ax5.set_yticklabels([f"{metric[0]}: {metric[1]}" for metric in quality_metrics])
        ax5.set_xlim(0, 1)
        ax5.set_title('æ•°æ®è´¨é‡æ£€æŸ¥')
        ax5.set_xticks([])

        # 6. é‡‡æ ·ç­–ç•¥æ•ˆæœ
        ax6 = axes[1, 2]
        sampling_info = validation_results['sampling_info']

        # åˆ›å»ºé¥¼å›¾æ˜¾ç¤ºé‡‡æ ·æ¯”ä¾‹
        sizes = [sampling_info['sampled_points'],
                sampling_info['original_points'] - sampling_info['sampled_points']]
        labels = ['é‡‡æ ·ç‚¹', 'æœªé‡‡æ ·ç‚¹']
        colors = ['lightblue', 'lightgray']

        wedges, texts, autotexts = ax6.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                                          startangle=90)
        ax6.set_title(f'é‡‡æ ·ç­–ç•¥: {sampling_info["strategy"]}\n'
                     f'é‡‡æ ·ç‡: {sampling_info["sampling_ratio"]*100:.1f}%')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ“ˆ éªŒè¯å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

    def compare_datasets(self, filenames: List[str]) -> Dict:
        """
        æ¯”è¾ƒå¤šä¸ªæ•°æ®é›†

        Args:
            filenames: æ•°æ®é›†æ–‡ä»¶ååˆ—è¡¨

        Returns:
            dict: æ¯”è¾ƒç»“æœ
        """
        print("ğŸ“Š æ¯”è¾ƒå¤šä¸ªæ•°æ®é›†...")

        comparison_results = {}

        for filename in filenames:
            report, data = self.generate_validation_report(filename)
            comparison_results[filename] = {
                'report': report,
                'key_metrics': {
                    'sampling_strategy': report['validation_results']['sampling_info']['strategy'],
                    'sampling_ratio': report['validation_results']['sampling_info']['sampling_ratio'],
                    'missing_ratio': report['validation_results']['missing_data']['missing_ratio'],
                    'max_speed': report['validation_results']['velocity_range']['speed_max'],
                    'pressure_range': report['validation_results']['pressure_range']['range']
                }
            }

        print("âœ… æ•°æ®é›†æ¯”è¾ƒå®Œæˆ")
        return comparison_results


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®éªŒè¯æµç¨‹"""
    print("ğŸŒŸ çœŸå®æ•°æ®é›†éªŒè¯å’Œåˆ†æå·¥å…·")

    # åˆ›å»ºéªŒè¯å™¨
    validator = RealisticDataValidator()

    # æŸ¥æ‰¾æ‰€æœ‰çœŸå®æ•°æ®é›†æ–‡ä»¶
    realistic_files = list(validator.data_dir.glob("realistic_data_*.h5"))

    if not realistic_files:
        print("âŒ æœªæ‰¾åˆ°çœŸå®æ•°æ®é›†æ–‡ä»¶")
        return

    print(f"ğŸ“ æ‰¾åˆ° {len(realistic_files)} ä¸ªçœŸå®æ•°æ®é›†æ–‡ä»¶")

    # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”ŸæˆéªŒè¯æŠ¥å‘Š
    all_reports = {}

    for file_path in realistic_files:
        filename = file_path.name
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ éªŒè¯æ•°æ®é›†: {filename}")
        print(f"{'='*60}")

        try:
            # ç”ŸæˆéªŒè¯æŠ¥å‘Š
            report, data = validator.generate_validation_report(filename)
            all_reports[filename] = report

            # æ‰“å°å…³é”®ä¿¡æ¯
            validation = report['validation_results']

            print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   æ–‡ä»¶å¤§å°: {report['dataset_info']['file_size_mb']} MB")
            print(f"   é‡‡æ ·ç­–ç•¥: {validation['sampling_info']['strategy']}")
            print(f"   é‡‡æ ·ç‡: {validation['sampling_info']['sampling_ratio']*100:.1f}%")
            print(f"   ç¼ºå¤±æ•°æ®: {validation['missing_data']['missing_ratio']*100:.1f}%")

            print(f"\nğŸ” ç‰©ç†é‡èŒƒå›´:")
            print(f"   æœ€å¤§é€Ÿåº¦: {validation['velocity_range']['speed_max']:.6f} m/s")
            print(f"   å‹åŠ›èŒƒå›´: {validation['pressure_range']['range']:.1f} Pa")

            if 'noise_levels' in validation:
                print(f"\nğŸ“ˆ å™ªå£°æ°´å¹³:")
                for field, noise_info in validation['noise_levels'].items():
                    print(f"   {field}: SNR = {noise_info['snr_db']:.1f} dB")

            print(f"\nâœ… æ•°æ®è´¨é‡æ£€æŸ¥:")
            print(f"   æ•°æ®å®Œæ•´æ€§: {(1-validation['missing_data']['missing_ratio'])*100:.1f}%")
            print(f"   é€Ÿåº¦åˆç†æ€§: {'âœ“' if validation['velocity_range']['reasonable'] else 'âœ—'}")
            print(f"   å‹åŠ›åˆç†æ€§: {'âœ“' if validation['pressure_range']['reasonable'] else 'âœ—'}")
            print(f"   å½¢çŠ¶ä¸€è‡´æ€§: {'âœ“' if validation['shape_consistency']['consistent'] else 'âœ—'}")

            # ç”ŸæˆéªŒè¯å¯è§†åŒ–
            vis_path = validator.data_dir / f"validation_report_{filename.replace('.h5', '.png')}"
            validator.visualize_data_validation(data, validation, str(vis_path))

        except Exception as e:
            print(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    # ç”Ÿæˆæ•°æ®é›†æ¯”è¾ƒæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š æ•°æ®é›†æ¯”è¾ƒæ€»ç»“")
    print(f"{'='*60}")

    if len(all_reports) > 1:
        filenames = list(all_reports.keys())
        comparison = validator.compare_datasets(filenames)

        print(f"\nğŸ“‹ å…³é”®æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æ–‡ä»¶å':<40} {'ç­–ç•¥':<15} {'é‡‡æ ·ç‡':<10} {'ç¼ºå¤±ç‡':<10} {'æœ€å¤§é€Ÿåº¦':<12}")
        print("-" * 90)

        for filename, metrics in comparison.items():
            key_metrics = metrics['key_metrics']
            print(f"{filename:<40} "
                  f"{key_metrics['sampling_strategy']:<15} "
                  f"{key_metrics['sampling_ratio']*100:>8.1f}% "
                  f"{key_metrics['missing_ratio']*100:>8.1f}% "
                  f"{key_metrics['max_speed']:>10.6f}")

    print(f"\nâœ… æ‰€æœ‰æ•°æ®é›†éªŒè¯å®Œæˆï¼")
    print(f"ğŸ“‚ éªŒè¯æŠ¥å‘Šä¿å­˜åœ¨: {validator.data_dir}")


if __name__ == "__main__":
    main()