"""
COMSOLæ•°æ®åŠ è½½å™¨

ä¸ºPINNsè®­ç»ƒæä¾›æ•°æ®åŠ è½½å’Œé¢„å¤„ç†åŠŸèƒ½
æ”¯æŒä»HDF5æ–‡ä»¶åŠ è½½COMSOLæ¨¡æ‹Ÿæ•°æ®

ä½œè€…: PINNsé¡¹ç›®ç»„
åˆ›å»ºæ—¶é—´: 2025-11-19
"""

import os
import sys
import numpy as np
import h5py
from pathlib import Path
from typing import Dict, Tuple, Optional
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))


class COMSOLDataLoader:
    """COMSOLæ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º comsol_simulation/data
        """
        if data_dir is None:
            data_dir = project_root / "comsol_simulation" / "data"
        
        self.data_dir = Path(data_dir)
        self.current_data = None
        
    def load_hdf5_data(self, filename: str) -> Dict:
        """
        ä»HDF5æ–‡ä»¶åŠ è½½COMSOLæ•°æ®
        
        Args:
            filename: HDF5æ–‡ä»¶å
            
        Returns:
            dict: åŒ…å«æ‰€æœ‰æ•°æ®çš„å­—å…¸
        """
        file_path = self.data_dir / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {filename}")
        
        try:
            with h5py.File(file_path, 'r') as h5file:
                data = {}
                
                # åŠ è½½åŸºæœ¬ä¿¡æ¯
                info_group = h5file.get('info')
                if info_group:
                    data['info'] = dict(info_group.attrs)
                
                # åŠ è½½ç½‘æ ¼æ•°æ®
                mesh_group = h5file.get('mesh')
                if mesh_group:
                    data['mesh'] = {
                        'x': mesh_group['x'][:],
                        'y': mesh_group['y'][:],
                        'num_nodes': mesh_group.attrs['num_nodes']
                    }
                
                # åŠ è½½æ±‚è§£æ•°æ®
                solution_group = h5file.get('solution')
                if solution_group:
                    data['solution'] = {
                        'u': solution_group['u'][:],
                        'v': solution_group['v'][:],
                        'p': solution_group['p'][:]
                    }
                
                # åŠ è½½ç»Ÿè®¡ä¿¡æ¯
                stats_group = h5file.get('statistics')
                if stats_group:
                    data['statistics'] = {}
                    for field in ['u', 'v', 'p']:
                        if field in stats_group:
                            data['statistics'][field] = dict(stats_group[field].attrs)
                
                self.current_data = data
                print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
                return data
                
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_training_data(self, data: Optional[Dict] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        è·å–PINNsè®­ç»ƒæ‰€éœ€çš„æ•°æ®æ ¼å¼
        
        Args:
            data: æ•°æ®å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰åŠ è½½çš„æ•°æ®
            
        Returns:
            tuple: (è¾“å…¥æ•°æ®, è¾“å‡ºæ•°æ®)
        """
        if data is None:
            if self.current_data is None:
                raise ValueError("æ²¡æœ‰åŠ è½½çš„æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨load_hdf5_data")
            data = self.current_data
        
        # æå–åæ ‡
        x = data['mesh']['x'].reshape(-1, 1)  # (N, 1)
        y = data['mesh']['y'].reshape(-1, 1)  # (N, 1)
        
        # æå–æµåœºæ•°æ®
        u = data['solution']['u'].reshape(-1, 1)  # (N, 1)
        v = data['solution']['v'].reshape(-1, 1)  # (N, 1)
        p = data['solution']['p'].reshape(-1, 1)  # (N, 1)
        
        # ç»„åˆè¾“å…¥ (x, yåæ ‡)
        X_train = np.hstack([x, y])  # (N, 2)
        
        # ç»„åˆè¾“å‡º (u, v, p)
        Y_train = np.hstack([u, v, p])  # (N, 3)
        
        print(f"ğŸ“Š è®­ç»ƒæ•°æ®æ ¼å¼:")
        print(f"   è¾“å…¥å½¢çŠ¶: {X_train.shape} (x, y)")
        print(f"   è¾“å‡ºå½¢çŠ¶: {Y_train.shape} (u, v, p)")
        
        return X_train, Y_train
    
    def normalize_data(self, X: np.ndarray, Y: np.ndarray, 
                      method: str = 'minmax') -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        æ•°æ®å½’ä¸€åŒ–
        
        Args:
            X: è¾“å…¥æ•°æ® (N, 2)
            Y: è¾“å‡ºæ•°æ® (N, 3)
            method: å½’ä¸€åŒ–æ–¹æ³• ('minmax' æˆ– 'standard')
            
        Returns:
            tuple: (å½’ä¸€åŒ–X, å½’ä¸€åŒ–Y, å½’ä¸€åŒ–å‚æ•°)
        """
        print(f"ğŸ”§ æ•°æ®å½’ä¸€åŒ– (æ–¹æ³•: {method})")
        
        normalization_params = {}
        
        if method == 'minmax':
            # Min-Maxå½’ä¸€åŒ–åˆ°[0, 1]
            x_min, x_max = X.min(axis=0), X.max(axis=0)
            y_min, y_max = Y.min(axis=0), Y.max(axis=0)
            
            X_norm = (X - x_min) / (x_max - x_min)
            Y_norm = (Y - y_min) / (y_max - y_min)
            
            normalization_params = {
                'method': 'minmax',
                'x_min': x_min,
                'x_max': x_max,
                'y_min': y_min,
                'y_max': y_max
            }
            
        elif method == 'standard':
            # æ ‡å‡†åŒ– (å‡å€¼0, æ ‡å‡†å·®1)
            x_mean, x_std = X.mean(axis=0), X.std(axis=0)
            y_mean, y_std = Y.mean(axis=0), Y.std(axis=0)
            
            X_norm = (X - x_mean) / x_std
            Y_norm = (Y - y_mean) / y_std
            
            normalization_params = {
                'method': 'standard',
                'x_mean': x_mean,
                'x_std': x_std,
                'y_mean': y_mean,
                'y_std': y_std
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {method}")
        
        print(f"âœ… å½’ä¸€åŒ–å®Œæˆ")
        return X_norm, Y_norm, normalization_params
    
    def denormalize_data(self, X_norm: np.ndarray, Y_norm: np.ndarray, 
                        params: Dict) -> Tuple[np.ndarray, np.ndarray]:
        """
        åå½’ä¸€åŒ–æ•°æ®
        
        Args:
            X_norm: å½’ä¸€åŒ–çš„è¾“å…¥æ•°æ®
            Y_norm: å½’ä¸€åŒ–çš„è¾“å‡ºæ•°æ®
            params: å½’ä¸€åŒ–å‚æ•°
            
        Returns:
            tuple: (åŸå§‹X, åŸå§‹Y)
        """
        method = params['method']
        
        if method == 'minmax':
            X = X_norm * (params['x_max'] - params['x_min']) + params['x_min']
            Y = Y_norm * (params['y_max'] - params['y_min']) + params['y_min']
        elif method == 'standard':
            X = X_norm * params['x_std'] + params['x_mean']
            Y = Y_norm * params['y_std'] + params['y_mean']
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {method}")
        
        return X, Y
    
    def split_data(self, X: np.ndarray, Y: np.ndarray, 
                  train_ratio: float = 0.7, val_ratio: float = 0.15,
                  random_seed: int = 42) -> Dict[str, np.ndarray]:
        """
        æ•°æ®é›†åˆ†å‰²
        
        Args:
            X: è¾“å…¥æ•°æ®
            Y: è¾“å‡ºæ•°æ®
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            random_seed: éšæœºç§å­
            
        Returns:
            dict: åŒ…å«è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®çš„å­—å…¸
        """
        np.random.seed(random_seed)
        
        N = len(X)
        indices = np.random.permutation(N)
        
        train_end = int(N * train_ratio)
        val_end = int(N * (train_ratio + val_ratio))
        
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]
        test_indices = indices[val_end:]
        
        data_split = {
            'X_train': X[train_indices],
            'Y_train': Y[train_indices],
            'X_val': X[val_indices],
            'Y_val': Y[val_indices],
            'X_test': X[test_indices],
            'Y_test': Y[test_indices]
        }
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ ({train_ratio*100:.1f}%)")
        print(f"   éªŒè¯é›†: {len(val_indices)} æ ·æœ¬ ({val_ratio*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ ({(1-train_ratio-val_ratio)*100:.1f}%)")
        
        return data_split
    
    def save_processed_data(self, data_split: Dict[str, np.ndarray], 
                          filename: str, normalization_params: Dict):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            data_split: åˆ†å‰²åçš„æ•°æ®å­—å…¸
            filename: ä¿å­˜æ–‡ä»¶å
            normalization_params: å½’ä¸€åŒ–å‚æ•°
        """
        output_path = self.data_dir / f"processed_{filename}"
        
        print(f"ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {output_path}")
        
        try:
            with h5py.File(output_path, 'w') as h5file:
                # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
                norm_group = h5file.create_group('normalization')
                for key, value in normalization_params.items():
                    if isinstance(value, np.ndarray):
                        norm_group.create_dataset(key, data=value)
                    else:
                        norm_group.attrs[key] = value
                
                # ä¿å­˜æ•°æ®é›†
                for key, value in data_split.items():
                    h5file.create_dataset(key, data=value)
                
                # æ·»åŠ å…ƒæ•°æ®
                h5file.attrs['creation_time'] = str(np.datetime64('now'))
                h5file.attrs['description'] = 'Processed data for PINN training'
            
            print(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def create_batch_generator(self, X: np.ndarray, Y: np.ndarray, 
                             batch_size: int = 32, shuffle: bool = True):
        """
        åˆ›å»ºæ‰¹é‡æ•°æ®ç”Ÿæˆå™¨
        
        Args:
            X: è¾“å…¥æ•°æ®
            Y: è¾“å‡ºæ•°æ®
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            
        Yields:
            tuple: (batch_X, batch_Y)
        """
        N = len(X)
        indices = np.arange(N)
        
        if shuffle:
            np.random.shuffle(indices)
        
        for start_idx in range(0, N, batch_size):
            end_idx = min(start_idx + batch_size, N)
            batch_indices = indices[start_idx:end_idx]
            
            yield X[batch_indices], Y[batch_indices]
    
    def visualize_data_distribution(self, X: np.ndarray, Y: np.ndarray, 
                                  save_path: Optional[str] = None):
        """
        å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
        
        Args:
            X: è¾“å…¥æ•°æ®
            Y: è¾“å‡ºæ•°æ®
            save_path: ä¿å­˜è·¯å¾„
        """
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        fig.suptitle('COMSOLæ•°æ®åˆ†å¸ƒ', fontsize=16)
        
        # åæ ‡åˆ†å¸ƒ
        axes[0, 0].scatter(X[:, 0], X[:, 1], s=1, alpha=0.6)
        axes[0, 0].set_xlabel('X (mm)')
        axes[0, 0].set_ylabel('Y (mm)')
        axes[0, 0].set_title('åæ ‡åˆ†å¸ƒ')
        axes[0, 0].set_aspect('equal')
        
        # é€Ÿåº¦åˆ†é‡åˆ†å¸ƒ
        axes[0, 1].hist(Y[:, 0], bins=50, alpha=0.7, label='u')
        axes[0, 1].set_xlabel('u (m/s)')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].set_title('Xæ–¹å‘é€Ÿåº¦åˆ†å¸ƒ')
        axes[0, 1].legend()
        
        axes[0, 2].hist(Y[:, 1], bins=50, alpha=0.7, label='v', color='orange')
        axes[0, 2].set_xlabel('v (m/s)')
        axes[0, 2].set_ylabel('é¢‘æ¬¡')
        axes[0, 2].set_title('Yæ–¹å‘é€Ÿåº¦åˆ†å¸ƒ')
        axes[0, 2].legend()
        
        # å‹åŠ›åˆ†å¸ƒ
        axes[1, 0].hist(Y[:, 2], bins=50, alpha=0.7, label='p', color='red')
        axes[1, 0].set_xlabel('p (Pa)')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        axes[1, 0].set_title('å‹åŠ›åˆ†å¸ƒ')
        axes[1, 0].legend()
        
        # é€Ÿåº¦åœº
        speed = np.sqrt(Y[:, 0]**2 + Y[:, 1]**2)
        scatter = axes[1, 1].scatter(X[:, 0], X[:, 1], c=speed, s=1, cmap='viridis')
        axes[1, 1].set_xlabel('X (mm)')
        axes[1, 1].set_ylabel('Y (mm)')
        axes[1, 1].set_title('é€Ÿåº¦å¹…å€¼')
        axes[1, 1].set_aspect('equal')
        plt.colorbar(scatter, ax=axes[1, 1])
        
        # å‹åŠ›åœº
        scatter2 = axes[1, 2].scatter(X[:, 0], X[:, 1], c=Y[:, 2], s=1, cmap='coolwarm')
        axes[1, 2].set_xlabel('X (mm)')
        axes[1, 2].set_ylabel('Y (mm)')
        axes[1, 2].set_title('å‹åŠ›åœº')
        axes[1, 2].set_aspect('equal')
        plt.colorbar(scatter2, ax=axes[1, 2])
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ“ˆ æ•°æ®åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
        else:
            plt.show()


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®åŠ è½½å’Œå¤„ç†æµç¨‹"""
    print("ğŸŒŸ COMSOLæ•°æ®åŠ è½½å™¨æ¼”ç¤º")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    loader = COMSOLDataLoader()
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    h5_files = list(loader.data_dir.glob("*.h5"))
    
    if not h5_files:
        print("âŒ æœªæ‰¾åˆ°HDF5æ•°æ®æ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    data_file = h5_files[-1].name
    print(f"ğŸ“ ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_file}")
    
    try:
        # 1. åŠ è½½æ•°æ®
        data = loader.load_hdf5_data(data_file)
        
        # 2. è·å–è®­ç»ƒæ•°æ®
        X, Y = loader.get_training_data()
        
        # 3. æ•°æ®å½’ä¸€åŒ–
        X_norm, Y_norm, norm_params = loader.normalize_data(X, Y, method='minmax')
        
        # 4. æ•°æ®åˆ†å‰²
        data_split = loader.split_data(X_norm, Y_norm)
        
        # 5. ä¿å­˜å¤„ç†åçš„æ•°æ®
        output_filename = f"processed_{data_file}"
        loader.save_processed_data(data_split, output_filename, norm_params)
        
        # 6. å¯è§†åŒ–æ•°æ®åˆ†å¸ƒ
        plot_path = loader.data_dir / "data_distribution.png"
        loader.visualize_data_distribution(X, Y, save_path=str(plot_path))
        
        # 7. æµ‹è¯•æ‰¹é‡ç”Ÿæˆå™¨
        print("\nğŸ§ª æµ‹è¯•æ‰¹é‡ç”Ÿæˆå™¨:")
        for i, (batch_X, batch_Y) in enumerate(loader.create_batch_generator(
                data_split['X_train'], data_split['Y_train'], batch_size=16)):
            print(f"   æ‰¹æ¬¡ {i+1}: X shape={batch_X.shape}, Y shape={batch_Y.shape}")
            if i >= 2:  # åªæµ‹è¯•å‰å‡ ä¸ªæ‰¹æ¬¡
                break
        
        print("\nâœ… æ•°æ®å¤„ç†æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸš€ æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºPINNsè®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()