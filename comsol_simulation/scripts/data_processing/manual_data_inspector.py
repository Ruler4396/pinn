"""
äººå·¥æ•°æ®é›†æ£€æŸ¥å·¥å…·

æä¾›ç®€å•ç›´è§‚çš„æ–¹å¼æ¥äººå·¥æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®é›†è´¨é‡å’ŒçœŸå®æ€§
åŒ…å«äº¤äº’å¼æŸ¥çœ‹å’Œè¯¦ç»†ç»Ÿè®¡åˆ†æåŠŸèƒ½

ä½œè€…: PINNsé¡¹ç›®ç»„
åˆ›å»ºæ—¶é—´: 2025-11-19
"""

import os
import sys
import numpy as np
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, Optional
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


class ManualDataInspector:
    """äººå·¥æ•°æ®æ£€æŸ¥å™¨"""

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–æ£€æŸ¥å™¨

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        if data_dir is None:
            data_dir = project_root / "comsol_simulation" / "data"

        self.data_dir = Path(data_dir)
        print(f"ğŸ“‚ æ•°æ®æ£€æŸ¥å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")

    def list_available_datasets(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†

        Returns:
            list: æ•°æ®é›†æ–‡ä»¶ååˆ—è¡¨
        """
        h5_files = list(self.data_dir.glob("*.h5"))
        realistic_files = [f for f in h5_files if "realistic" in f.name]

        print(f"\nğŸ“‹ å‘ç° {len(realistic_files)} ä¸ªçœŸå®æ•°æ®é›†:")
        for i, file_path in enumerate(realistic_files, 1):
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            print(f"   {i}. {file_path.name} ({size_mb:.2f} MB)")

        return realistic_files

    def load_dataset_simple(self, filename: str) -> Dict:
        """
        ç®€å•åŠ è½½æ•°æ®é›†

        Args:
            filename: æ•°æ®é›†æ–‡ä»¶å

        Returns:
            dict: ç®€åŒ–çš„æ•°æ®å­—å…¸
        """
        file_path = self.data_dir / filename

        with h5py.File(file_path, 'r') as h5file:
            # åŸºæœ¬ä¿¡æ¯
            info = {}
            if 'info' in h5file:
                info = dict(h5file['info'].attrs)

            # ç½‘æ ¼å’Œåæ ‡
            x = h5file['mesh']['x'][:]
            y = h5file['mesh']['y'][:]

            # å¹²å‡€æ•°æ®å’Œå™ªå£°æ•°æ®
            solution_data = {}
            if 'solution' in h5file:
                sol = h5file['solution']
                solution_data = {
                    'x': x,
                    'y': y,
                    'u_clean': sol['u_clean'][:],
                    'v_clean': sol['v_clean'][:],
                    'p_clean': sol['p_clean'][:],
                    'u_noisy': sol['u'][:],
                    'v_noisy': sol['v'][:],
                    'p_noisy': sol['p'][:]
                }

                # å¦‚æœæœ‰ç¼ºå¤±æ•°æ®æ©ç 
                if 'missing_mask' in sol:
                    solution_data['missing_mask'] = sol['missing_mask'][:]

            # å™ªå£°åˆ†æ
            noise_info = {}
            if 'noise_analysis' in h5file:
                noise_group = h5file['noise_analysis']
                for field in ['u', 'v', 'p']:
                    if field in noise_group:
                        noise_info[field] = dict(noise_group[field].attrs)

        return {
            'filename': filename,
            'info': info,
            'data': solution_data,
            'noise_analysis': noise_info
        }

    def print_basic_info(self, dataset: Dict):
        """
        æ‰“å°åŸºæœ¬ä¿¡æ¯

        Args:
            dataset: æ•°æ®é›†å­—å…¸
        """
        print(f"\n{'='*50}")
        print(f"ğŸ“‹ æ•°æ®é›†åŸºæœ¬ä¿¡æ¯: {dataset['filename']}")
        print(f"{'='*50}")

        # åŸºæœ¬ä¿¡æ¯
        info = dataset['info']
        print(f"ğŸ“… åˆ›å»ºæ—¶é—´: {info.get('creation_time', 'æœªçŸ¥')}")
        print(f"ğŸ”¬ æ•°æ®ç±»å‹: {info.get('data_type', 'æœªçŸ¥')}")
        print(f"ğŸ“Š æè¿°: {info.get('description', 'æ— ')}")

        # æ•°æ®ç»Ÿè®¡
        data = dataset['data']
        n_points = len(data['x'])
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   æ•°æ®ç‚¹æ•°: {n_points}")
        print(f"   XèŒƒå›´: {np.min(data['x']):.3f} ~ {np.max(data['x']):.3f} mm")
        print(f"   YèŒƒå›´: {np.min(data['y']):.3f} ~ {np.max(data['y']):.3f} mm")

        # ç‰©ç†é‡ç»Ÿè®¡
        speed_clean = np.sqrt(data['u_clean']**2 + data['v_clean']**2)
        speed_noisy = np.sqrt(data['u_noisy']**2 + data['v_noisy']**2)

        print(f"\nğŸ”¬ ç‰©ç†é‡ç»Ÿè®¡:")
        print(f"   Xæ–¹å‘é€Ÿåº¦ (å¹²å‡€): {np.min(data['u_clean']):.6f} ~ {np.max(data['u_clean']):.6f} m/s")
        print(f"   Yæ–¹å‘é€Ÿåº¦ (å¹²å‡€): {np.min(data['v_clean']):.6f} ~ {np.max(data['v_clean']):.6f} m/s")
        print(f"   é€Ÿåº¦å¹…å€¼ (å¹²å‡€): {np.min(speed_clean):.6f} ~ {np.max(speed_clean):.6f} m/s")
        print(f"   å‹åŠ› (å¹²å‡€): {np.min(data['p_clean']):.1f} ~ {np.max(data['p_clean']):.1f} Pa")

        print(f"   Xæ–¹å‘é€Ÿåº¦ (å™ªå£°): {np.min(data['u_noisy']):.6f} ~ {np.max(data['u_noisy']):.6f} m/s")
        print(f"   Yæ–¹å‘é€Ÿåº¦ (å™ªå£°): {np.min(data['v_noisy']):.6f} ~ {np.max(data['v_noisy']):.6f} m/s")
        print(f"   é€Ÿåº¦å¹…å€¼ (å™ªå£°): {np.min(speed_noisy):.6f} ~ {np.max(speed_noisy):.6f} m/s")
        print(f"   å‹åŠ› (å™ªå£°): {np.min(data['p_noisy']):.1f} ~ {np.max(data['p_noisy']):.1f} Pa")

        # å™ªå£°åˆ†æ
        if dataset['noise_analysis']:
            print(f"\nğŸ“Š å™ªå£°åˆ†æ:")
            for field, noise_data in dataset['noise_analysis'].items():
                snr = noise_data.get('snr_db', 'N/A')
                std = noise_data.get('noise_std', 'N/A')
                print(f"   {field}åœº: SNR = {snr} dB, å™ªå£°æ ‡å‡†å·® = {std}")

    def show_raw_data_samples(self, dataset: Dict, n_samples: int = 10):
        """
        æ˜¾ç¤ºåŸå§‹æ•°æ®æ ·æœ¬

        Args:
            dataset: æ•°æ®é›†å­—å…¸
            n_samples: æ˜¾ç¤ºæ ·æœ¬æ•°é‡
        """
        print(f"\nğŸ” åŸå§‹æ•°æ®æ ·æœ¬ (å‰{n_samples}ä¸ªç‚¹):")
        print(f"{'åºå·':<4} {'X(mm)':<10} {'Y(mm)':<10} {'U(m/s)':<12} {'V(m/s)':<12} {'P(Pa)':<12}")
        print("-" * 70)

        data = dataset['data']
        for i in range(min(n_samples, len(data['x']))):
            print(f"{i+1:<4} "
                  f"{data['x'][i]:<10.3f} "
                  f"{data['y'][i]:<10.3f} "
                  f"{data['u_noisy'][i]:<12.6f} "
                  f"{data['v_noisy'][i]:<12.6f} "
                  f"{data['p_noisy'][i]:<12.1f}")

        # å¦‚æœæ•°æ®ç‚¹å¾ˆå¤šï¼Œæ˜¾ç¤ºæœ€åå‡ ä¸ª
        if len(data['x']) > n_samples:
            print("   ...")
            for i in range(max(0, len(data['x'])-3), len(data['x'])):
                print(f"{i+1:<4} "
                      f"{data['x'][i]:<10.3f} "
                      f"{data['y'][i]:<10.3f} "
                      f"{data['u_noisy'][i]:<12.6f} "
                      f"{data['v_noisy'][i]:<12.6f} "
                      f"{data['p_noisy'][i]:<12.1f}")

    def visualize_data_overview(self, dataset: Dict, save_path: Optional[str] = None):
        """
        å¯è§†åŒ–æ•°æ®æ¦‚è§ˆ

        Args:
            dataset: æ•°æ®é›†å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        data = dataset['data']
        x, y = data['x'], data['y']

        # è®¡ç®—é€Ÿåº¦å¹…å€¼
        speed_clean = np.sqrt(data['u_clean']**2 + data['v_clean']**2)
        speed_noisy = np.sqrt(data['u_noisy']**2 + data['v_noisy']**2)

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'æ•°æ®é›†æ¦‚è§ˆ: {dataset["filename"]}', fontsize=16)

        # 1. é‡‡æ ·ç‚¹åˆ†å¸ƒ
        ax1 = axes[0, 0]
        scatter = ax1.scatter(x, y, c=speed_noisy, s=20, cmap='viridis', alpha=0.8)
        ax1.set_xlabel('X (mm)')
        ax1.set_ylabel('Y (mm)')
        ax1.set_title('æ•°æ®ç‚¹åˆ†å¸ƒ (é¢œè‰²=é€Ÿåº¦)')
        ax1.set_aspect('equal')
        plt.colorbar(scatter, ax=ax1, label='é€Ÿåº¦ (m/s)')

        # 2. é€Ÿåº¦åœºå¯¹æ¯”
        ax2 = axes[0, 1]
        ax2.scatter(data['u_clean'], data['u_noisy'], alpha=0.6, s=10, label='Uåˆ†é‡')
        ax2.scatter(data['v_clean'], data['v_noisy'], alpha=0.6, s=10, label='Våˆ†é‡')
        max_vel = max(np.max(np.abs(data['u_clean'])), np.max(np.abs(data['v_clean'])))
        ax2.plot([-max_vel, max_vel], [-max_vel, max_vel], 'r--', alpha=0.8, label='ç†æƒ³åŒ¹é…')
        ax2.set_xlabel('å¹²å‡€æ•°æ® (m/s)')
        ax2.set_ylabel('å™ªå£°æ•°æ® (m/s)')
        ax2.set_title('é€Ÿåº¦åœºå™ªå£°å½±å“')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. å‹åŠ›åœºå¯¹æ¯”
        ax3 = axes[0, 2]
        ax3.scatter(data['p_clean'], data['p_noisy'], alpha=0.6, s=10, c='orange')
        min_p, max_p = min(np.min(data['p_clean']), np.min(data['p_noisy'])), max(np.max(data['p_clean']), np.max(data['p_noisy']))
        ax3.plot([min_p, max_p], [min_p, max_p], 'r--', alpha=0.8, label='ç†æƒ³åŒ¹é…')
        ax3.set_xlabel('å¹²å‡€å‹åŠ› (Pa)')
        ax3.set_ylabel('å™ªå£°å‹åŠ› (Pa)')
        ax3.set_title('å‹åŠ›åœºå™ªå£°å½±å“')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. é€Ÿåº¦å¹…å€¼ç›´æ–¹å›¾
        ax4 = axes[1, 0]
        ax4.hist(speed_clean, bins=30, alpha=0.7, label='å¹²å‡€æ•°æ®', density=True)
        ax4.hist(speed_noisy, bins=30, alpha=0.7, label='å™ªå£°æ•°æ®', density=True)
        ax4.set_xlabel('é€Ÿåº¦å¹…å€¼ (m/s)')
        ax4.set_ylabel('æ¦‚ç‡å¯†åº¦')
        ax4.set_title('é€Ÿåº¦å¹…å€¼åˆ†å¸ƒ')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # 5. å‹åŠ›ç›´æ–¹å›¾
        ax5 = axes[1, 1]
        ax5.hist(data['p_clean'], bins=30, alpha=0.7, label='å¹²å‡€æ•°æ®', density=True)
        ax5.hist(data['p_noisy'], bins=30, alpha=0.7, label='å™ªå£°æ•°æ®', density=True)
        ax5.set_xlabel('å‹åŠ› (Pa)')
        ax5.set_ylabel('æ¦‚ç‡å¯†åº¦')
        ax5.set_title('å‹åŠ›åˆ†å¸ƒ')
        ax5.legend()
        ax5.grid(True, alpha=0.3)

        # 6. è¯¯å·®åˆ†æ
        ax6 = axes[1, 2]
        u_error = np.abs(data['u_noisy'] - data['u_clean'])
        v_error = np.abs(data['v_noisy'] - data['v_clean'])
        p_error = np.abs(data['p_noisy'] - data['p_clean'])

        ax6.hist(u_error, bins=20, alpha=0.7, label=f'Uè¯¯å·® (å‡å€¼:{np.mean(u_error):.2e})')
        ax6.hist(v_error, bins=20, alpha=0.7, label=f'Vè¯¯å·® (å‡å€¼:{np.mean(v_error):.2e})')
        ax6.hist(p_error, bins=20, alpha=0.7, label=f'Pè¯¯å·® (å‡å€¼:{np.mean(p_error):.1f})')
        ax6.set_xlabel('ç»å¯¹è¯¯å·®')
        ax6.set_ylabel('é¢‘æ¬¡')
        ax6.set_title('æµ‹é‡è¯¯å·®åˆ†æ')
        ax6.legend()
        ax6.grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"ğŸ“ˆ æ•°æ®æ¦‚è§ˆå›¾å·²ä¿å­˜: {save_path}")
        else:
            plt.show()

    def check_physical_consistency(self, dataset: Dict):
        """
        æ£€æŸ¥ç‰©ç†ä¸€è‡´æ€§

        Args:
            dataset: æ•°æ®é›†å­—å…¸
        """
        print(f"\nğŸ”¬ ç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥:")

        data = dataset['data']
        x, y = data['x'], data['y']
        u, v = data['u_noisy'], data['v_noisy']
        p = data['p_noisy']

        # 1. æ£€æŸ¥é€Ÿåº¦èŒƒå›´çš„åˆç†æ€§
        speed = np.sqrt(u**2 + v**2)
        max_speed = np.max(speed)
        avg_speed = np.mean(speed)

        print(f"   ğŸ“Š é€Ÿåº¦åˆ†æ:")
        print(f"      æœ€å¤§é€Ÿåº¦: {max_speed:.6f} m/s")
        print(f"      å¹³å‡é€Ÿåº¦: {avg_speed:.6f} m/s")
        print(f"      é€Ÿåº¦èŒƒå›´åˆç†æ€§: {'âœ“' if max_speed < 0.1 else 'âš ï¸'} (å¾®æµæ§é€šå¸¸ < 0.1 m/s)")

        # 2. æ£€æŸ¥å‹åŠ›é™çš„åˆç†æ€§
        min_pressure = np.min(p)
        max_pressure = np.max(p)
        pressure_drop = max_pressure - min_pressure

        print(f"   ğŸ“Š å‹åŠ›åˆ†æ:")
        print(f"      å‹åŠ›èŒƒå›´: {min_pressure:.1f} ~ {max_pressure:.1f} Pa")
        print(f"      å‹åŠ›é™: {pressure_drop:.1f} Pa")
        print(f"      å‹åŠ›èŒƒå›´åˆç†æ€§: {'âœ“' if pressure_drop < 50000 else 'âš ï¸'} (å¾®æµæ§é€šå¸¸ < 50 kPa)")

        # 3. ä¼°ç®—é›·è¯ºæ•°
        # å‡è®¾ç‰¹å¾å°ºå¯¸ä¸ºé€šé“å®½åº¦ (çº¦0.2mm = 2e-4 m)ï¼Œæ°´ä¸ºå·¥è´¨
        channel_width = 0.2e-3  # m
        kinematic_viscosity = 1e-6  # mÂ²/s (æ°´)
        reynolds_number = avg_speed * channel_width / kinematic_viscosity

        print(f"   ğŸ“Š æµåŠ¨ç‰¹å¾:")
        print(f"      ä¼°ç®—é›·è¯ºæ•°: {reynolds_number:.1f}")
        print(f"      æµåŠ¨çŠ¶æ€: {'å±‚æµ' if reynolds_number < 2300 else 'æ¹æµ'}")

        # 4. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if 'missing_mask' in data:
            missing_ratio = np.mean(data['missing_mask']) * 100
            print(f"   ğŸ“Š æ•°æ®å®Œæ•´æ€§:")
            print(f"      ç¼ºå¤±æ•°æ®æ¯”ä¾‹: {missing_ratio:.1f}%")
            print(f"      æ•°æ®è´¨é‡: {'ä¼˜ç§€' if missing_ratio < 2 else 'è‰¯å¥½' if missing_ratio < 5 else 'ä¸€èˆ¬'}")

        # 5. æ£€æŸ¥å™ªå£°æ°´å¹³
        noise_analysis = dataset['noise_analysis']
        if noise_analysis:
            print(f"   ğŸ“Š å™ªå£°æ°´å¹³:")
            for field, noise_data in noise_analysis.items():
                snr = noise_data.get('snr_db', 0)
                if snr > 40:
                    quality = "ä¼˜ç§€"
                elif snr > 30:
                    quality = "è‰¯å¥½"
                elif snr > 20:
                    quality = "ä¸€èˆ¬"
                else:
                    quality = "è¾ƒå·®"
                print(f"      {field}åœºä¿¡å™ªæ¯”: {snr:.1f} dB ({quality})")

        print(f"\nğŸ“‹ æ€»ä½“è¯„ä¼°:")
        issues = []
        if max_speed >= 0.1:
            issues.append("é€Ÿåº¦èŒƒå›´å¯èƒ½ä¸åˆç†")
        if pressure_drop >= 50000:
            issues.append("å‹åŠ›é™å¯èƒ½è¿‡é«˜")
        if reynolds_number >= 2300:
            issues.append("å¯èƒ½ä¸æ˜¯å±‚æµçŠ¶æ€")

        if not issues:
            print("   âœ… æ•°æ®é›†ç‰©ç†ç‰¹å¾åˆç†ï¼Œé€‚åˆPINNsè®­ç»ƒ")
        else:
            print("   âš ï¸ å‘ç°ä»¥ä¸‹æ½œåœ¨é—®é¢˜:")
            for issue in issues:
                print(f"      - {issue}")

    def interactive_inspection(self, filename: str):
        """
        äº¤äº’å¼æ•°æ®æ£€æŸ¥

        Args:
            filename: æ•°æ®é›†æ–‡ä»¶å
        """
        print(f"\nğŸ” å¼€å§‹äº¤äº’å¼æ£€æŸ¥: {filename}")

        # åŠ è½½æ•°æ®é›†
        dataset = self.load_dataset_simple(filename)

        # 1. æ‰“å°åŸºæœ¬ä¿¡æ¯
        self.print_basic_info(dataset)

        # 2. æ˜¾ç¤ºåŸå§‹æ•°æ®æ ·æœ¬
        self.show_raw_data_samples(dataset, n_samples=15)

        # 3. ç‰©ç†ä¸€è‡´æ€§æ£€æŸ¥
        self.check_physical_consistency(dataset)

        # 4. å¯è§†åŒ–æ•°æ®æ¦‚è§ˆ
        vis_path = self.data_dir / f"manual_inspection_{filename.replace('.h5', '.png')}"
        self.visualize_data_overview(dataset, str(vis_path))

        # 5. å¯¼å‡ºCSVæ–‡ä»¶ç”¨äºExcelæŸ¥çœ‹
        csv_path = self.data_dir / f"inspection_data_{filename.replace('.h5', '.csv')}"
        self.export_to_csv(dataset, str(csv_path))

        print(f"\nâœ… äº¤äº’å¼æ£€æŸ¥å®Œæˆ!")
        print(f"ğŸ“Š å¯è§†åŒ–å›¾: {vis_path}")
        print(f"ğŸ“„ CSVæ–‡ä»¶: {csv_path}")

    def export_to_csv(self, dataset: Dict, csv_path: str):
        """
        å¯¼å‡ºæ•°æ®åˆ°CSVæ–‡ä»¶

        Args:
            dataset: æ•°æ®é›†å­—å…¸
            csv_path: CSVæ–‡ä»¶è·¯å¾„
        """
        data = dataset['data']
        df = pd.DataFrame({
            'X_mm': data['x'],
            'Y_mm': data['y'],
            'U_clean_m_s': data['u_clean'],
            'V_clean_m_s': data['v_clean'],
            'P_clean_Pa': data['p_clean'],
            'U_noisy_m_s': data['u_noisy'],
            'V_noisy_m_s': data['v_noisy'],
            'P_noisy_Pa': data['p_noisy']
        })

        # å¦‚æœæœ‰ç¼ºå¤±æ•°æ®æ©ç ï¼Œæ·»åŠ åˆ°CSV
        if 'missing_mask' in data:
            df['Is_Missing'] = data['missing_mask'].astype(int)

        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ æ•°æ®å·²å¯¼å‡ºåˆ°CSV: {csv_path}")


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼æ•°æ®æ£€æŸ¥"""
    print("ğŸŒŸ äººå·¥æ•°æ®é›†æ£€æŸ¥å·¥å…·")

    # åˆ›å»ºæ£€æŸ¥å™¨
    inspector = ManualDataInspector()

    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    datasets = inspector.list_available_datasets()

    if not datasets:
        print("âŒ æœªæ‰¾åˆ°çœŸå®æ•°æ®é›†æ–‡ä»¶")
        return

    print(f"\né€‰æ‹©è¦æ£€æŸ¥çš„æ•°æ®é›†:")
    for i, dataset_path in enumerate(datasets, 1):
        print(f"   {i}. {dataset_path.name}")

    # è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œæ£€æŸ¥ï¼ˆé¿å…äº¤äº’å¼è¾“å…¥é—®é¢˜ï¼‰
    print(f"\nğŸ” è‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œè¯¦ç»†æ£€æŸ¥...")
    selected_dataset = datasets[0]

    # æ‰§è¡Œäº¤äº’å¼æ£€æŸ¥
    inspector.interactive_inspection(selected_dataset.name)

    # å¯é€‰ï¼šæ£€æŸ¥å…¶ä»–æ•°æ®é›†
    if len(datasets) > 1:
        print(f"\nğŸ“‹ ç®€è¦æ£€æŸ¥å…¶ä»– {len(datasets)-1} ä¸ªæ•°æ®é›†...")
        for i, dataset_path in enumerate(datasets[1:], 2):
            print(f"\n{'='*50}")
            print(f"ğŸ“‹ å¿«é€Ÿæ£€æŸ¥æ•°æ®é›† {i}: {dataset_path.name}")
            print(f"{'='*50}")

              try:
                dataset = inspector.load_dataset_simple(dataset_path.name)
                inspector.print_basic_info(dataset)
                inspector.check_physical_consistency(dataset)
            except Exception as e:
                print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")


if __name__ == "__main__":
    main()